{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "current_path = pathlib.Path().resolve().parent\n",
    "print(current_path)\n",
    "\n",
    "def find_pt_file(folder_path):\n",
    "    # List all files in the folder\n",
    "    all_files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter out the .pt files\n",
    "    pt_filename = [f for f in all_files if f.endswith('.pt')][0]\n",
    "\n",
    "    return int(pt_filename.split(\".\")[0].split('_')[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'distilbert'\n",
    "corpus = 'ibm'\n",
    "task = 'binary'\n",
    "\n",
    "results_path = current_path / 'results' / f\"{corpus}-{model}-{task}/\"\n",
    "\n",
    "macro_f1_scores = []\n",
    "accuracy_scores = [] \n",
    "\n",
    "names_list = ['No-PM', 'PM']\n",
    "\n",
    "scores_per_category = {x: [] for x in names_list}\n",
    "\n",
    "best_iter = find_pt_file(results_path)\n",
    "\n",
    "for exp in range(10):\n",
    "    df = pd.read_csv(results_path / f'test_results_exp_{exp}.csv')\n",
    "    true_labels = df['true_label']\n",
    "    predictions = df['prediction']\n",
    "\n",
    "    classes_scores = classification_report(true_labels, predictions, target_names = names_list, output_dict = True)\n",
    "\n",
    "    for cl in names_list:\n",
    "        scores_per_category[cl] = scores_per_category[cl] + [classes_scores[cl]['f1-score']]\n",
    "    \n",
    "    macro_f1 = f1_score(df['true_label'], df['prediction'], average='macro')\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "    \n",
    "    accuracy = accuracy_score(df['true_label'], df['prediction'])\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    if(exp == best_iter):\n",
    "        print(exp)\n",
    "        print(classification_report(true_labels, predictions, target_names=['No-PM', 'PM']))\n",
    "\n",
    "\n",
    "values = []\n",
    "for cl in ['PM', 'No-PM']:\n",
    "    f1s = scores_per_category[cl]\n",
    "    values.append(f\"{round(np.mean(f1s), 3)}$\\pm${round(np.std(f1s), 3)}\")\n",
    "\n",
    "values.append(f\"{round(np.mean(macro_f1_scores), 3)}$\\pm${round(np.std(macro_f1_scores), 3)}\")\n",
    "values.append(f\"{round(np.mean(accuracy_scores), 3)}$\\pm${round(np.std(accuracy_scores), 3)}\")\n",
    "\n",
    "output_string = \" & \".join(values)\n",
    "print(output_string)\n",
    "\n",
    "output_string = re.sub(r'(?<!\\d)0\\.', '.', output_string)\n",
    "print(output_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'bert'\n",
    "corpus = 'ukp'\n",
    "task = 'multi'\n",
    "results_path = current_path / 'results' / f\"{corpus}-{model}-{task}/\"\n",
    "\n",
    "best_iter = find_pt_file(results_path)\n",
    "print(best_iter)\n",
    "\n",
    "names_list = ['correct', 'flipped', 'neutralized', 'polarized']\n",
    "\n",
    "scores_per_category = {x: [] for x in names_list}\n",
    "\n",
    "macro_f1_scores = []\n",
    "accuracy_scores = [] \n",
    "\n",
    "for exp in range(10):\n",
    "    df = pd.read_csv(results_path / f'test_results_exp_{exp}.csv')\n",
    "    true_labels = df['true_label']\n",
    "    predictions = df['prediction']\n",
    "    classes_scores = classification_report(true_labels, predictions, target_names = names_list, output_dict = True)\n",
    "\n",
    "    for cl in names_list:\n",
    "        scores_per_category[cl] = scores_per_category[cl] + [classes_scores[cl]['f1-score']]\n",
    "    \n",
    "    macro_f1 = f1_score(df['true_label'], df['prediction'], average='macro')\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "    \n",
    "    accuracy = accuracy_score(df['true_label'], df['prediction'])\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    if exp == best_iter:\n",
    "        print(classification_report(true_labels, predictions, target_names=names_list))\n",
    "\n",
    "values = []\n",
    "for cl in ['correct', 'neutralized', 'polarized', 'flipped']:\n",
    "    f1s = scores_per_category[cl]\n",
    "    values.append(f\"{round(np.mean(f1s), 3)}$\\pm${round(np.std(f1s), 3)}\")\n",
    "\n",
    "values.append(f\"{round(np.mean(macro_f1_scores), 3)}$\\pm${round(np.std(macro_f1_scores), 3)}\")\n",
    "values.append(f\"{round(np.mean(accuracy_scores), 3)}$\\pm${round(np.std(accuracy_scores), 3)}\")\n",
    "\n",
    "output_string = \" & \".join(values)\n",
    "print(output_string)\n",
    "\n",
    "output_string = re.sub(r'(?<!\\d)0\\.', '.', output_string)\n",
    "print(output_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
